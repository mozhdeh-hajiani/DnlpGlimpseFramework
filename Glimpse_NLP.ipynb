{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l3V1ciDTs3ot",
        "lnL96Y6fAnR-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirments"
      ],
      "metadata": {
        "id": "uoc08_yQrpQ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES97218griTs"
      },
      "outputs": [],
      "source": [
        "#First time\n",
        "\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Navigate to your Google Drive directory and clone the repo there\n",
        "!git clone https://github.com/icannos/glimpse-mds.git /content/drive/MyDrive/glimpse/glimpse-mds\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accessing the Repository Later\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change to the cloned repository\n",
        "%cd /content/drive/MyDrive/glimpse/glimpse-mds"
      ],
      "metadata": {
        "id": "Uyksfrwhrtx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a53853f-c75e-4669-df79-afa6bb0966d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/glimpse/glimpse-mds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch with CUDA 12.1 support\n",
        "!pip install nltk datasets pandas tqdm\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu122\n",
        "!sudo apt-get install python3.10\n",
        "!pip install lxml[html_clean]\n",
        "!pip install sumy transformers rouge_score"
      ],
      "metadata": {
        "id": "2upNj1dkrtu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('all')  # To download all resources (use when doing the processing part)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch"
      ],
      "metadata": {
        "id": "3lnB3TxSrtsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv requirements requirements.txt\n",
        "# Install other dependencies\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "4YWcBZtDrtmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each file contains columns id, text (review), and gold (metareview)\n",
        "!python glimpse/data_loading/data_processing.py"
      ],
      "metadata": {
        "id": "lmD883_Irtgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extractive methods"
      ],
      "metadata": {
        "id": "l3V1ciDTs3ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base paths for processed data and output\n",
        "import os\n",
        "\n",
        "processed_dir = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/processed\"\n",
        "output_dir = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/candidates/extractive\"\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define the range of years to process\n",
        "years = range(2017, 2018)\n",
        "\n",
        "# Iterate over each year and run the script\n",
        "for year in years:\n",
        "    dataset_path = f\"{processed_dir}/all_reviews_{year}.csv\"\n",
        "    print(f\"Processing year {year}...\")\n",
        "\n",
        "    # Check if input file exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"Error: Input file {dataset_path} does not exist. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Run the script and check for errors\n",
        "    exit_code = os.system(f\"\"\"\n",
        "    python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/data_loading/generate_extractive_candidates.py \\\n",
        "        --dataset_path {dataset_path} \\\n",
        "        --output_dir {output_dir}\n",
        "    \"\"\")\n",
        "    if exit_code != 0:\n",
        "        print(f\"Error: Script failed for year {year} with exit code {exit_code}\")\n",
        "    else:\n",
        "        print(f\"Year {year} processed successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQuGO2A-rtd_",
        "outputId": "10d887b3-8cef-4415-8202-d129f6b5ba6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing year 2017...\n",
            "Year 2017 processed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/glimpse/glimpse-mds')\n",
        "from rsasumm.rsa_reranker import RSAReranking\n",
        "print(\"Import successful\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu_r4J1crta7",
        "outputId": "9c13c7ce-5915-4793-b452-e5e8918a3a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1 python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/src/compute_rsa.py \\\n",
        "            --model_name facebook/bart-large-cnn \\\n",
        "            --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/candidates/extractive/extractive_sentences-_-all_reviews_2017-_-none-_-2025-03-24-07-43-18.csv \\\n",
        "            --output_dir /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/rsa_scores \\\n",
        "            --device cuda"
      ],
      "metadata": {
        "id": "YTzuYFQ-rtYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_glimpse_summaries_from_rsa.py \\\n",
        "    --rsa_scores \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/rsa_scores/extractive/extractive_sentences-_-all_reviews_2017-_-none-_-2025-03-24-07-43-18-_-r3-_-rsa_reranked-facebook-bart-large-cnn.pk\" \\\n",
        "    --output_dir \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017\" \\\n",
        "    --year 2017 \\\n",
        "    --n_unique_sentences 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZGjXMj6rtWE",
        "outputId": "f355cb8c-1da0-47b5-f53b-98b17bdee3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading RSA scores from /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/rsa_scores/extractive/extractive_sentences-_-all_reviews_2017-_-none-_-2025-03-24-07-43-18-_-r3-_-rsa_reranked-facebook-bart-large-cnn.pk...\n",
            "\n",
            "Generating summaries for 490 documents...\n",
            "100% 490/490 [00:00<00:00, 3576.72it/s]\n",
            "\n",
            "Summary Statistics:\n",
            "GLIMPSE-Speaker summaries: 486\n",
            "GLIMPSE-Unique summaries: 490\n",
            "Average speaker summary length: 173.70 chars\n",
            "Average unique summary length: 208.37 chars\n",
            "\n",
            "Saved summaries to:\n",
            "- /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/speaker_summaries_2017.csv\n",
            "- /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/unique_summaries_2017.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Original file paths\n",
        "src_path1 = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/speaker_summaries_2017.csv\"\n",
        "src_path2 = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/unique_summaries_2017.csv\"\n",
        "\n",
        "# New file paths with new names\n",
        "dst_path1 = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/rough_speaker_summaries_2017.csv\"\n",
        "dst_path2 = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/rough_unique_summaries_2017.csv\"\n",
        "\n",
        "# Copy and rename both files\n",
        "shutil.copy(src_path1, dst_path1)\n",
        "shutil.copy(src_path2, dst_path2)\n",
        "\n",
        "# Print confirmation\n",
        "print(f\"File copied to: {dst_path1}\")\n",
        "print(f\"File copied to: {dst_path2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE7gUpom2I1c",
        "outputId": "5d9947be-53a0-499f-bd4c-4b8f55fa976d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File copied to: /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/rough_speaker_summaries_2017.csv\n",
            "File copied to: /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/rough_unique_summaries_2017.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "speaker_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/rough_speaker_summaries_2017.csv\"\n",
        "unique_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/rough_unique_summaries_2017.csv\"\n",
        "\n",
        "# Read and modify speaker summaries\n",
        "speaker_df = pd.read_csv(speaker_path)\n",
        "speaker_df = speaker_df.rename(columns={'text': 'gold'})\n",
        "speaker_df.to_csv(speaker_path, index=False)\n",
        "\n",
        "# Read and modify unique summaries\n",
        "unique_df = pd.read_csv(unique_path)\n",
        "unique_df = unique_df.rename(columns={'text': 'gold'})\n",
        "unique_df.to_csv(unique_path, index=False)\n",
        "\n",
        "# Verify the changes\n",
        "print(\"Speaker summaries columns:\", speaker_df.columns.tolist())\n",
        "print(\"Unique summaries columns:\", unique_df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcObF25q1nCV",
        "outputId": "c74c5aeb-31cd-4a5e-859f-cc6098a67b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker summaries columns: ['id', 'Method', 'summary', 'gold']\n",
            "Unique summaries columns: ['id', 'Method', 'summary', 'gold']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/rough_speaker_summaries_2017.csv\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/rough_unique_summaries_2017.csv\n"
      ],
      "metadata": {
        "id": "ZG0Ns1SI3Pnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to the updated CSVs after running ROUGE evaluation\n",
        "output_speaker = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/rough_speaker_summaries_2017.csv\"\n",
        "output_unique = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/rough_unique_summaries_2017.csv\"\n",
        "\n",
        "# Load CSV files\n",
        "df_speaker = pd.read_csv(output_speaker)\n",
        "df_unique = pd.read_csv(output_unique)\n",
        "\n",
        "# Check if ROUGE scores exist\n",
        "print(\"Columns in GLIMPSE-Speaker CSV:\", df_speaker.columns)\n",
        "print(\"Columns in GLIMPSE-Unique CSV:\", df_unique.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAkxtcQr3aCe",
        "outputId": "c47c273c-3b88-486d-8db8-f4bd1ca0ffb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in GLIMPSE-Speaker CSV: Index(['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2',\n",
            "       'common/rougeL', 'common/rougeLsum'],\n",
            "      dtype='object')\n",
            "Columns in GLIMPSE-Unique CSV: Index(['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2',\n",
            "       'common/rougeL', 'common/rougeLsum'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only ROUGE score columns\n",
        "rouge_columns = [\"common/rouge1\", \"common/rouge2\", \"common/rougeL\", \"common/rougeLsum\"]\n",
        "\n",
        "# Compute mean and std ROUGE scores (as decimals, not percentages)\n",
        "mean_speaker = df_speaker[rouge_columns].mean()  # Removed * 100\n",
        "std_speaker = df_speaker[rouge_columns].std()    # Removed * 100\n",
        "mean_unique = df_unique[rouge_columns].mean()    # Removed * 100\n",
        "std_unique = df_unique[rouge_columns].std()      # Removed * 100\n",
        "\n",
        "# Create a formatted table\n",
        "print(\"\\nROUGE Scores:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<12} {'GLIMPSE-Speaker':>22} {'GLIMPSE-Unique':>22}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "metrics_display = {\n",
        "    \"common/rouge1\": \"ROUGE-1\",\n",
        "    \"common/rouge2\": \"ROUGE-2\",\n",
        "    \"common/rougeL\": \"ROUGE-L\",\n",
        "    \"common/rougeLsum\": \"ROUGE-Lsum\"\n",
        "}\n",
        "\n",
        "for col in rouge_columns:\n",
        "    metric_name = metrics_display[col]\n",
        "    # Format scores to match paper style (0.XX ±0.XX)\n",
        "    speaker_score = f\"{mean_speaker[col]:.2f} ±{std_speaker[col]:.2f}\"\n",
        "    unique_score = f\"{mean_unique[col]:.2f} ±{std_unique[col]:.2f}\"\n",
        "    print(f\"{metric_name:<12} {speaker_score:>22} {unique_score:>22}\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfipNcEl3Z_4",
        "outputId": "355757bf-2aac-4890-826b-a39c366c8713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores:\n",
            "============================================================\n",
            "Metric              GLIMPSE-Speaker         GLIMPSE-Unique\n",
            "------------------------------------------------------------\n",
            "ROUGE-1                  0.17 ±0.07             0.18 ±0.07\n",
            "ROUGE-2                  0.02 ±0.02             0.03 ±0.03\n",
            "ROUGE-L                  0.12 ±0.05             0.12 ±0.04\n",
            "ROUGE-Lsum               0.12 ±0.05             0.12 ±0.04\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Original file paths\n",
        "src_path1 = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/speaker_summaries_2017.csv\"\n",
        "src_path2 = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/unique_summaries_2017.csv\"\n",
        "\n",
        "# New file paths with new names\n",
        "dst_path1 = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/seahorse_speaker_summaries_2017.csv\"\n",
        "dst_path2 = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/seahorse_unique_summaries_2017.csv\"\n",
        "\n",
        "# Copy and rename both files\n",
        "shutil.copy(src_path1, dst_path1)\n",
        "shutil.copy(src_path2, dst_path2)\n",
        "\n",
        "# Print confirmation\n",
        "print(f\"File copied to: {dst_path1}\")\n",
        "print(f\"File copied to: {dst_path2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vQ5lHNF3Z9R",
        "outputId": "aff9f33c-12f7-42e3-e48b-bd1d22e149fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File copied to: /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/seahorse_speaker_summaries_2017.csv\n",
            "File copied to: /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/seahorse_unique_summaries_2017.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sh = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/seahorse_unique_summaries_2017.csv\")\n",
        "print(df_sh.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CaJ971M3Z6d",
        "outputId": "4e657920-0178-49b8-8819-de0054692405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'Method', 'summary', 'text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to the CSV file\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/seahorse_speaker_summaries_2017.csv\"\n",
        "file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/seahorse_unique_summaries_2017.csv\"\n",
        "\n",
        "for question in range(1, 7):\n",
        "    print(f\"⚡ Running Seahorse for Question {question}...\")\n",
        "\n",
        "    # Reload dataset before each run to avoid missing columns\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        print(\"❌ ERROR: 'text' and 'summary' columns missing before running Seahorse. Stopping execution.\")\n",
        "        break  # Stop if important columns are missing\n",
        "\n",
        "    # Run the Seahorse evaluation script\n",
        "    !python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_seahorse_metrics_samples.py \\\n",
        "        --summaries {file_path} \\\n",
        "        --question {question} \\\n",
        "        --batch_size 8 \\\n",
        "        --device cuda\n",
        "\n",
        "    # ✅ Verify columns after running Seahorse\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"✅ After Question {question}, Columns:\", df.columns.tolist())\n",
        "\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        print(\"❌ ERROR: 'text' and 'summary' columns missing after running Seahorse. Stopping execution.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "jrBrt19I3Z4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the speaker summaries file\n",
        "#df_seahorse = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/seahorse_speaker_summaries_2017.csv\")\n",
        "df_seahorse = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/extractive/seahorse_unique_summaries_2017.csv\")\n",
        "\n",
        "# Get SEAHORSE metrics (only the proba_1 columns)\n",
        "seahorse_cols = [col for col in df_seahorse.columns if col.startswith(\"SHMetric\") and col.endswith(\"proba_1\")]\n",
        "\n",
        "print(\"\\nSEAHORSE Metrics for GLIMPSE:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if seahorse_cols:\n",
        "    for col in seahorse_cols:\n",
        "        metric_name = col.split(\"/\")[1]\n",
        "        mean_score = df_seahorse[col].mean()\n",
        "        std_score = df_seahorse[col].std()\n",
        "        print(f\"{metric_name}: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "else:\n",
        "    print(\"No SEAHORSE metrics found in the file.\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Print some basic statistics\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(f\"Number of summaries: {len(df_seahorse)}\")\n",
        "print(f\"Average summary length: {df_seahorse['summary'].str.len().mean():.2f} characters\")"
      ],
      "metadata": {
        "id": "OUvMZuu53Z1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline- extractive methods"
      ],
      "metadata": {
        "id": "YiI3p14G8w8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For LSA\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/baselines/sumy_baselines.py \\\n",
        "  --dataset /content/drive/MyDrive/glimpse/glimpse-mds/data/processed/all_reviews_2017.csv \\\n",
        "  --method LSA \\\n",
        "  --output /content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LSA\n",
        "\n",
        "# For LexRank\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/baselines/sumy_baselines.py \\\n",
        "  --dataset /content/drive/MyDrive/glimpse/glimpse-mds/data/processed/all_reviews_2017.csv \\\n",
        "  --method lex-rank \\\n",
        "  --output /content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LexRank\n",
        "\n",
        "\n",
        "# For Random\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/baselines/sumy_baselines.py \\\n",
        "  --dataset /content/drive/MyDrive/glimpse/glimpse-mds/data/processed/all_reviews_2017.csv \\\n",
        "  --method random \\\n",
        "  --output /content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/Random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VMKDOL78xjX",
        "outputId": "1299b49f-9a8f-4879-9912-2be5fd387e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sample summaries generated:\n",
            "                                                text                                            summary\n",
            "0  Summary: The paper presents low-rank bilinear ...  In the caption for Table 1, fix the following:...\n",
            "1  Results on the VQA task are good for this simp...  Results on the VQA task are good for this simp...\n",
            "2  This work proposes to approximate the bilinear...  Although the evaluated model is similar to Fuk...\n",
            "3  Summary:--------This paper proposes to use sur...  This claim is wrong.--------The model requires...\n",
            "4  This paper proposes to use previous error sign...  This paper proposes to use previous error sign...\n",
            "✅ Summaries saved at: /content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LSA/all_reviews_2017-_-LSA-_-sumy_1.csv\n",
            "✅ Sample summaries generated:\n",
            "                                                text                                            summary\n",
            "0  Summary: The paper presents low-rank bilinear ...  Hence, it could not be experimentally verified...\n",
            "1  Results on the VQA task are good for this simp...  If you increase embedded and output dimensions...\n",
            "2  This work proposes to approximate the bilinear...  It is a bit unfortunate that most of the exper...\n",
            "3  Summary:--------This paper proposes to use sur...  Authors have shown a result on language modeli...\n",
            "4  This paper proposes to use previous error sign...  There are already more than four papers report...\n",
            "✅ Summaries saved at: /content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LexRank/all_reviews_2017-_-lex-rank-_-sumy_1.csv\n",
            "✅ Sample summaries generated:\n",
            "                                                text                                            summary\n",
            "0  Summary: The paper presents low-rank bilinear ...  Summary: The paper presents low-rank bilinear ...\n",
            "1  Results on the VQA task are good for this simp...  If you increase embedded and output dimensions...\n",
            "2  This work proposes to approximate the bilinear...  Related work: The comparison to the related wo...\n",
            "3  Summary:--------This paper proposes to use sur...  I assume that authors do that since they claim...\n",
            "4  This paper proposes to use previous error sign...  This paper proposes to use previous error sign...\n",
            "✅ Summaries saved at: /content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/Random/all_reviews_2017-_-random-_-sumy_1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_rough_file = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/Random/all_reviews_2017-_-random-_-sumy_1.csv\"\n",
        "\n",
        "df_rough = pd.read_csv(input_rough_file)\n",
        "# Check if ROUGE scores exist\n",
        "print(\"Columns in df_rough CSV:\", df_rough.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnqvceGr9LjM",
        "outputId": "5e40e77f-8ca4-428c-dcc1-5f4a61c9cef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in df_rough CSV: Index(['id', 'text', 'gold', 'summary', 'metadata/dataset', 'metadata/method',\n",
            "       'metadata/sentence_count'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# rough scores for LSA method\n",
        "#!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LSA/rough_all_reviews_2017-_-LSA-_-sumy_1.csv\n",
        "\n",
        "# rough scores for lex_Rank method\n",
        "#!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LexRank/rough_all_reviews_2017-_-lex-rank-_-sumy_1.csv\n",
        "\n",
        "# rough scores for Random method\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/Random/rough_all_reviews_2017-_-random-_-sumy_1.csv"
      ],
      "metadata": {
        "id": "z_aWChwW9OYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rough_file = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LSA/rough_all_reviews_2017-_-LSA-_-sumy_1.csv\"\n",
        "rough_file = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LexRank/rough_all_reviews_2017-_-lex-rank-_-sumy_1.csv\"\n",
        "#rough_file = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/Random/rough_all_reviews_2017-_-random-_-sumy_1.csv\"\n",
        "\n",
        "df_rough = pd.read_csv(rough_file)\n",
        "# Check if ROUGE scores exist\n",
        "print(\"Columns in df_rough CSV:\", df_rough.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekvk1i-E9OVm",
        "outputId": "1dd6798a-9182-4c55-fe07-ca3bb72c1cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in df_rough CSV: Index(['id', 'text', 'gold', 'summary', 'metadata/dataset', 'metadata/method',\n",
            "       'metadata/sentence_count', 'common/rouge1', 'common/rouge2',\n",
            "       'common/rougeL', 'common/rougeLsum'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only ROUGE score columns\n",
        "rouge_columns = [\"common/rouge1\", \"common/rouge2\", \"common/rougeL\", \"common/rougeLsum\"]\n",
        "\n",
        "# Compute mean and std ROUGE scores (as decimals, not percentages)\n",
        "mean_scores = df_rough[rouge_columns].mean()  # Remove the * 100\n",
        "std_scores = df_rough[rouge_columns].std()    # Remove the * 100\n",
        "\n",
        "# Create a formatted table\n",
        "method_name = df_rough['metadata/method'].iloc[0]\n",
        "print(f\"\\nROUGE Scores for {method_name}:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"{'Metric':<12} {'Score':>22}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "metrics_display = {\n",
        "    \"common/rouge1\": \"ROUGE-1\",\n",
        "    \"common/rouge2\": \"ROUGE-2\",\n",
        "    \"common/rougeL\": \"ROUGE-L\",\n",
        "    \"common/rougeLsum\": \"ROUGE-Lsum\"\n",
        "}\n",
        "\n",
        "for col in rouge_columns:\n",
        "    metric_name = metrics_display[col]\n",
        "    # Format scores to match paper style (0.XX ±0.XX)\n",
        "    score = f\"{mean_scores[col]:.2f} ±{std_scores[col]:.2f}\"\n",
        "    print(f\"{metric_name:<12} {score:>22}\")\n",
        "\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Print additional statistics\n",
        "print(f\"\\nBasic Statistics:\")\n",
        "print(f\"Number of summaries: {len(df_rough)}\")\n",
        "print(f\"Average summary length: {df_rough['summary'].str.len().mean():.0f} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ie3tAxV9OTA",
        "outputId": "eae31855-6bc0-43b2-cb7d-f6b3c54a408d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores for lex-rank:\n",
            "========================================\n",
            "Metric                        Score\n",
            "----------------------------------------\n",
            "ROUGE-1                  0.17 ±0.09\n",
            "ROUGE-2                  0.02 ±0.03\n",
            "ROUGE-L                  0.11 ±0.05\n",
            "ROUGE-Lsum               0.11 ±0.05\n",
            "========================================\n",
            "\n",
            "Basic Statistics:\n",
            "Number of summaries: 1511\n",
            "Average summary length: 227 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/Random/seahorse_all_reviews_2017-_-random-_-sumy_1.csv\")\n",
        "\n",
        "raw_df.head()\n",
        "print(raw_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv7zNKme9OQK",
        "outputId": "06eb9d81-a79c-4ee8-c185-9afd81853c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'method', 'summary', 'text', 'SHMetric/Comprehensible/proba_1',\n",
            "       'SHMetric/Comprehensible/proba_0', 'SHMetric/Comprehensible/guess',\n",
            "       'SHMetric/Repetition/proba_1', 'SHMetric/Repetition/proba_0',\n",
            "       'SHMetric/Repetition/guess', 'SHMetric/Grammar/proba_1',\n",
            "       'SHMetric/Grammar/proba_0', 'SHMetric/Grammar/guess',\n",
            "       'SHMetric/Attribution/proba_1', 'SHMetric/Attribution/proba_0',\n",
            "       'SHMetric/Attribution/guess', 'SHMetric/Main ideas/proba_1',\n",
            "       'SHMetric/Main ideas/proba_0', 'SHMetric/Main ideas/guess',\n",
            "       'SHMetric/Conciseness/proba_1', 'SHMetric/Conciseness/proba_0',\n",
            "       'SHMetric/Conciseness/guess'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the CSV file\n",
        "file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/Random/seahorse_all_reviews_2017-_-random-_-sumy_1.csv\"\n",
        "\n",
        "# Read the original file and keep only required columns\n",
        "df = pd.read_csv(file_path)\n",
        "df = df[['id', 'metadata/method', 'summary', 'text']]\n",
        "df = df.rename(columns={'metadata/method': 'method'})\n",
        "\n",
        "# Print sample of the data to verify\n",
        "print(\"Sample of input data:\")\n",
        "print(df.head())\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nNumber of rows:\", len(df))"
      ],
      "metadata": {
        "id": "gWLTOfzv9ON0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Path to the CSV file\n",
        "\n",
        "# LSA method\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LSA/seahorse_all_reviews_2017-_-LSA-_-sumy_1.csv\"\n",
        "\n",
        "#lex_rank method\n",
        "file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LexRank/seahorse_all_reviews_2017-_-lex-rank-_-sumy_1.csv\"\n",
        "\n",
        "#Random method\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/Random/seahorse_all_reviews_2017-_-random-_-sumy_1.csv\"\n",
        "\n",
        "# Read the original file and keep only required columns\n",
        "df = pd.read_csv(file_path)\n",
        "df = df[['id', 'metadata/method', 'summary', 'text']]\n",
        "df = df.rename(columns={'metadata/method': 'method'})\n",
        "\n",
        "# Create a temporary file in the same directory\n",
        "temp_file = os.path.join(os.path.dirname(file_path), 'temp_seahorse_eval.csv')\n",
        "df.to_csv(temp_file, index=False)\n",
        "\n",
        "# Now run SEAHORSE evaluation on the temporary file\n",
        "for question in range(1, 7):\n",
        "    print(f\"\\n⚡ Running Seahorse for Question {question}...\")\n",
        "\n",
        "    # Run the Seahorse evaluation script on temp file\n",
        "    cmd = f\"python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_seahorse_metrics_samples.py --summaries {temp_file} --question {question} --batch_size 8 --device cuda\"\n",
        "\n",
        "    try:\n",
        "        # Run command and capture output\n",
        "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "        # Print command output\n",
        "        print(f\"Command output:\\n{result.stdout}\")\n",
        "\n",
        "        if result.stderr:\n",
        "            print(f\"Errors:\\n{result.stderr}\")\n",
        "\n",
        "        # Check if command was successful\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Command failed with return code {result.returncode}\")\n",
        "            continue\n",
        "\n",
        "        # Load and check the results after each question\n",
        "        temp_df = pd.read_csv(temp_file)\n",
        "        seahorse_cols = [col for col in temp_df.columns if 'SHMetric' in col]\n",
        "\n",
        "        if seahorse_cols:\n",
        "            print(f\"✅ After Question {question}, Found SEAHORSE columns:\")\n",
        "            for col in seahorse_cols:\n",
        "                non_nan = temp_df[col].notna().sum()\n",
        "                print(f\"  - {col}: {non_nan} non-NaN values\")\n",
        "        else:\n",
        "            print(f\"❌ No SEAHORSE columns found after Question {question}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running question {question}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# After all questions are done, merge results back to original file\n",
        "try:\n",
        "    final_df = pd.read_csv(temp_file)\n",
        "\n",
        "    # Get all SEAHORSE metric columns\n",
        "    seahorse_columns = [col for col in final_df.columns if 'SHMetric' in col]\n",
        "\n",
        "    if not seahorse_columns:\n",
        "        print(\"❌ No SEAHORSE metrics were calculated!\")\n",
        "    else:\n",
        "        print(f\"\\nFound {len(seahorse_columns)} SEAHORSE metric columns\")\n",
        "\n",
        "        # Add SEAHORSE metrics to original DataFrame\n",
        "        for col in seahorse_columns:\n",
        "            df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
        "\n",
        "        # Print summary of metrics\n",
        "        print(\"\\nSummary of SEAHORSE metrics:\")\n",
        "        for col in seahorse_columns:\n",
        "            non_nan = df[col].notna().sum()\n",
        "            print(f\"{col}: {non_nan} non-NaN values\")\n",
        "\n",
        "        # Save the updated original file\n",
        "        df.to_csv(file_path, index=False)\n",
        "        print(f\"\\n✅ Saved results to {file_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing final results: {str(e)}\")\n",
        "\n",
        "# Clean up temporary file\n",
        "try:\n",
        "    os.remove(temp_file)\n",
        "    print(\"✅ Cleaned up temporary file\")\n",
        "except Exception as e:\n",
        "    print(f\"Error removing temporary file: {str(e)}\")\n",
        "\n",
        "print(\"\\nFinal columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "6UbOA5og9OLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seahorse_scores = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LSA/seahorse_all_reviews_2017-_-LSA-_-sumy_1.csv\")\n",
        "seahorse_scores = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LexRank/seahorse_all_reviews_2017-_-lex-rank-_-sumy_1.csv\")\n",
        "#seahorse_scores = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/Random/seahorse_all_reviews_2017-_-random-_-sumy_1.csv\")\n",
        "\n",
        "seahorse_scores.head()\n",
        "print(seahorse_scores.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIYnJ-2J9OIm",
        "outputId": "b5f3a93e-537f-4616-90ac-ebaea7a7f8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'method', 'summary', 'text', 'SHMetric/Comprehensible/proba_1',\n",
            "       'SHMetric/Comprehensible/proba_0', 'SHMetric/Comprehensible/guess',\n",
            "       'SHMetric/Repetition/proba_1', 'SHMetric/Repetition/proba_0',\n",
            "       'SHMetric/Repetition/guess', 'SHMetric/Grammar/proba_1',\n",
            "       'SHMetric/Grammar/proba_0', 'SHMetric/Grammar/guess',\n",
            "       'SHMetric/Attribution/proba_1', 'SHMetric/Attribution/proba_0',\n",
            "       'SHMetric/Attribution/guess', 'SHMetric/Main ideas/proba_1',\n",
            "       'SHMetric/Main ideas/proba_0', 'SHMetric/Main ideas/guess',\n",
            "       'SHMetric/Conciseness/proba_1', 'SHMetric/Conciseness/proba_0',\n",
            "       'SHMetric/Conciseness/guess'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Seahorse scores CSV file\n",
        "\n",
        "# LSA method\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LSA/seahorse_all_reviews_2017-_-LSA-_-sumy_1.csv\"\n",
        "\n",
        "#lex_rank method\n",
        "file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/LexRank/seahorse_all_reviews_2017-_-lex-rank-_-sumy_1.csv\"\n",
        "\n",
        "#Random method\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/baseline_summaries/Random/seahorse_all_reviews_2017-_-random-_-sumy_1.csv\"\n",
        "\n",
        "\n",
        "seahorse_scores = pd.read_csv(file_path)\n",
        "\n",
        "# Select the relevant columns for calculation, including 'Main Ideas' as 'Coverage'\n",
        "columns_to_average = {\n",
        "    \"Coverage\": \"SHMetric/Main ideas/proba_1\",  # ✅ Added\n",
        "    \"Comprehensible\": \"SHMetric/Comprehensible/proba_1\",\n",
        "    \"Attribution\": \"SHMetric/Attribution/proba_1\",\n",
        "    \"Grammar\": \"SHMetric/Grammar/proba_1\",\n",
        "    \"Conciseness\": \"SHMetric/Conciseness/proba_1\",\n",
        "    \"Repetition\": \"SHMetric/Repetition/proba_1\"\n",
        "}\n",
        "\n",
        "# Compute mean ± std deviation for each metric\n",
        "summary_results = {}\n",
        "for metric, column in columns_to_average.items():\n",
        "    if column in seahorse_scores.columns:\n",
        "        mean_value = seahorse_scores[column].mean()\n",
        "        std_value = seahorse_scores[column].std()\n",
        "        summary_results[metric] = f\"{mean_value:.2f} ± {std_value:.2f}\"\n",
        "\n",
        "# Print the computed results\n",
        "summary_results\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCRAYKdq9OGA",
        "outputId": "d20e21cd-97be-48d3-8c7d-d7b9e0579c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Coverage': '0.26 ± 0.23',\n",
              " 'Comprehensible': '0.93 ± 0.10',\n",
              " 'Attribution': '0.93 ± 0.06',\n",
              " 'Grammar': '0.77 ± 0.20',\n",
              " 'Conciseness': '0.38 ± 0.17',\n",
              " 'Repetition': '0.97 ± 0.09'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstractive methods"
      ],
      "metadata": {
        "id": "lnL96Y6fAnR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base paths for processed data and output\n",
        "import os\n",
        "\n",
        "processed_dir = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/processed\"\n",
        "output_dir = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/candidates/abstractive\"\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "decoding_config = \"beam_search\"  # ✅ Changed to Beam Search\n",
        "batch_size = 8\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define the range of years to process\n",
        "years = range(2017, 2018)\n",
        "\n",
        "# Loop through each year and run the script\n",
        "for year in years:\n",
        "    dataset_path = f\"{processed_dir}/all_reviews_{year}.csv\"\n",
        "    !python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/data_loading/generate_abstractive_candidates.py \\\n",
        "        --model_name {model_name} \\\n",
        "        --dataset_path {dataset_path} \\\n",
        "        --output_dir {output_dir} \\\n",
        "        --batch_size {batch_size} \\\n",
        "        --decoding_config {decoding_config} \\\n",
        "        --no-trimming\n",
        "\n",
        "print(\"All years processed successfully!\")"
      ],
      "metadata": {
        "id": "Nzhmj-Od9ODZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/glimpse/glimpse-mds')\n",
        "from rsasumm.rsa_reranker import RSAReranking\n",
        "print(\"Import successful\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy87I6KB9OAx",
        "outputId": "ceaa697f-d28b-4e59-c5f3-ab55fef9202b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1 python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/src/compute_rsa.py \\\n",
        "            --model_name facebook/bart-large-cnn \\\n",
        "            --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/candidates/abstractive/facebook_bart-large-cnn-_-all_reviews_2017-_-beam_search-_-trimmed-_-2025-03-26-10-31-47.csv \\\n",
        "            --output_dir /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/rsa_scores/abstractive \\\n",
        "            --device cuda"
      ],
      "metadata": {
        "id": "xL3d4VowBhAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/generate_glimpse_summaries_from_rsa.py \\\n",
        "    --rsa_scores \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/rsa_scores/abstractive/facebook_bart-large-cnn-_-all_reviews_2017-_-beam_search-_-trimmed-_-2025-03-26-10-31-47-_-r3-_-rsa_reranked-facebook-bart-large-cnn.pk\" \\\n",
        "    --output_dir \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive\" \\\n",
        "    --year 2017 \\\n",
        "    --n_unique_sentences 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJMWYKHqBg-R",
        "outputId": "312a9880-5ddd-4b75-9bf2-2283effa5461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading RSA scores from /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/rsa_scores/abstractive/facebook_bart-large-cnn-_-all_reviews_2017-_-beam_search-_-trimmed-_-2025-03-26-10-31-47-_-r3-_-rsa_reranked-facebook-bart-large-cnn.pk...\n",
            "\n",
            "Generating summaries for 490 documents...\n",
            "100% 490/490 [00:00<00:00, 3965.45it/s]\n",
            "\n",
            "Summary Statistics:\n",
            "GLIMPSE-Speaker summaries: 488\n",
            "GLIMPSE-Unique summaries: 490\n",
            "Average speaker summary length: 229.41 chars\n",
            "Average unique summary length: 227.02 chars\n",
            "\n",
            "Saved summaries to:\n",
            "- /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/speaker_summaries_2017.csv\n",
            "- /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/unique_summaries_2017.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the generated files\n",
        "speaker_df = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/rough_speaker_summaries_2017.csv\")\n",
        "unique_df = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/rough_unique_summaries_2017.csv\")\n",
        "\n",
        "print(\"Speaker summaries columns:\", speaker_df.columns.tolist())\n",
        "print(\"Unique summaries columns:\", unique_df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8zI9vgEBg7-",
        "outputId": "296b6af7-aea5-4c14-ab15-1997c809b6b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker summaries columns: ['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2', 'common/rougeL', 'common/rougeLsum']\n",
            "Unique summaries columns: ['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2', 'common/rougeL', 'common/rougeLsum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "speaker_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/rough_speaker_summaries_2017.csv\"\n",
        "unique_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/rough_unique_summaries_2017.csv\"\n",
        "\n",
        "# Read and modify speaker summaries\n",
        "speaker_df = pd.read_csv(speaker_path)\n",
        "speaker_df = speaker_df.rename(columns={'text': 'gold'})\n",
        "speaker_df.to_csv(speaker_path, index=False)\n",
        "\n",
        "# Read and modify unique summaries\n",
        "unique_df = pd.read_csv(unique_path)\n",
        "unique_df = unique_df.rename(columns={'text': 'gold'})\n",
        "unique_df.to_csv(unique_path, index=False)\n",
        "\n",
        "# Verify the changes\n",
        "print(\"Speaker summaries columns:\", speaker_df.columns.tolist())\n",
        "print(\"Unique summaries columns:\", unique_df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXYYl6DGBg5U",
        "outputId": "cfb7ae77-8232-4e02-bc0c-12461b4731f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker summaries columns: ['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2', 'common/rougeL', 'common/rougeLsum']\n",
            "Unique summaries columns: ['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2', 'common/rougeL', 'common/rougeLsum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/rough_speaker_summaries_2017.csv\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/rough_unique_summaries_2017.csv\n"
      ],
      "metadata": {
        "id": "OF5wUgR0Bg2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to the updated CSVs after running ROUGE evaluation\n",
        "output_speaker = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/rough_speaker_summaries_2017.csv\"\n",
        "output_unique = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/rough_unique_summaries_2017.csv\"\n",
        "\n",
        "# Load CSV files\n",
        "df_speaker = pd.read_csv(output_speaker)\n",
        "df_unique = pd.read_csv(output_unique)\n",
        "\n",
        "# Check if ROUGE scores exist\n",
        "print(\"Columns in GLIMPSE-Speaker CSV:\", df_speaker.columns)\n",
        "print(\"Columns in GLIMPSE-Unique CSV:\", df_unique.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QMkrVz6Bg0L",
        "outputId": "e6cecf98-2c81-4b5e-c390-9436a98f1d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in GLIMPSE-Speaker CSV: Index(['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2',\n",
            "       'common/rougeL', 'common/rougeLsum'],\n",
            "      dtype='object')\n",
            "Columns in GLIMPSE-Unique CSV: Index(['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2',\n",
            "       'common/rougeL', 'common/rougeLsum'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only ROUGE score columns\n",
        "rouge_columns = [\"common/rouge1\", \"common/rouge2\", \"common/rougeL\", \"common/rougeLsum\"]\n",
        "\n",
        "# Compute mean and std ROUGE scores (as decimals, not percentages)\n",
        "mean_speaker = df_speaker[rouge_columns].mean()  # Removed * 100\n",
        "std_speaker = df_speaker[rouge_columns].std()    # Removed * 100\n",
        "mean_unique = df_unique[rouge_columns].mean()    # Removed * 100\n",
        "std_unique = df_unique[rouge_columns].std()      # Removed * 100\n",
        "\n",
        "# Create a formatted table\n",
        "print(\"\\nROUGE Scores:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<12} {'GLIMPSE-Speaker':>22} {'GLIMPSE-Unique':>22}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "metrics_display = {\n",
        "    \"common/rouge1\": \"ROUGE-1\",\n",
        "    \"common/rouge2\": \"ROUGE-2\",\n",
        "    \"common/rougeL\": \"ROUGE-L\",\n",
        "    \"common/rougeLsum\": \"ROUGE-Lsum\"\n",
        "}\n",
        "\n",
        "for col in rouge_columns:\n",
        "    metric_name = metrics_display[col]\n",
        "    # Format scores to match paper style (0.XX ±0.XX)\n",
        "    speaker_score = f\"{mean_speaker[col]:.2f} ±{std_speaker[col]:.2f}\"\n",
        "    unique_score = f\"{mean_unique[col]:.2f} ±{std_unique[col]:.2f}\"\n",
        "    print(f\"{metric_name:<12} {speaker_score:>22} {unique_score:>22}\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YaG76ALBgxw",
        "outputId": "e45e96ed-18e4-4e20-9327-72ff2ae9637f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores:\n",
            "============================================================\n",
            "Metric              GLIMPSE-Speaker         GLIMPSE-Unique\n",
            "------------------------------------------------------------\n",
            "ROUGE-1                  0.19 ±0.07             0.18 ±0.07\n",
            "ROUGE-2                  0.03 ±0.02             0.03 ±0.02\n",
            "ROUGE-L                  0.13 ±0.04             0.12 ±0.04\n",
            "ROUGE-Lsum               0.13 ±0.04             0.12 ±0.04\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sh = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/seahorse_unique_summaries_2017.csv\")\n",
        "print(df_sh.columns)"
      ],
      "metadata": {
        "id": "FjhNNfxXBgvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to the CSV file\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/seahorse_speaker_summaries_2017.csv\"\n",
        "file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/seahorse_unique_summaries_2017.csv\"\n",
        "\n",
        "for question in range(1, 7):\n",
        "    print(f\"⚡ Running Seahorse for Question {question}...\")\n",
        "\n",
        "    # Reload dataset before each run to avoid missing columns\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        print(\"❌ ERROR: 'text' and 'summary' columns missing before running Seahorse. Stopping execution.\")\n",
        "        break  # Stop if important columns are missing\n",
        "\n",
        "    # Run the Seahorse evaluation script\n",
        "    !python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_seahorse_metrics_samples.py \\\n",
        "        --summaries {file_path} \\\n",
        "        --question {question} \\\n",
        "        --batch_size 8 \\\n",
        "        --device cuda\n",
        "\n",
        "    # ✅ Verify columns after running Seahorse\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"✅ After Question {question}, Columns:\", df.columns.tolist())\n",
        "\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        print(\"❌ ERROR: 'text' and 'summary' columns missing after running Seahorse. Stopping execution.\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsyN8_iZBgsk",
        "outputId": "17f52309-d19f-4c2d-8df3-671cc6145307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Running Seahorse for Question 1...\n",
            "2025-06-12 16:53:19.013425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749747199.051540   26342 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749747199.063908   26342 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-12 16:53:19.105802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_seahorse_metrics_samples.py\", line 133, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_seahorse_metrics_samples.py\", line 104, in main\n",
            "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 571, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 309, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4574, in from_pretrained\n",
            "    ) = cls._load_pretrained_model(\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4833, in _load_pretrained_model\n",
            "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 572, in load_state_dict\n",
            "    return torch.load(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1462, in load\n",
            "    return _load(\n",
            "           ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1964, in _load\n",
            "    result = unpickler.load()\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_weights_only_unpickler.py\", line 512, in load\n",
            "    self.append(self.persistent_load(pid))\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1928, in persistent_load\n",
            "    typed_storage = load_tensor(\n",
            "                    ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1888, in load_tensor\n",
            "    zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "✅ After Question 1, Columns: ['id', 'Method', 'summary', 'text', 'SHMetric/Comprehensible/proba_1', 'SHMetric/Comprehensible/proba_0', 'SHMetric/Comprehensible/guess', 'SHMetric/Repetition/proba_1', 'SHMetric/Repetition/proba_0', 'SHMetric/Repetition/guess', 'SHMetric/Grammar/proba_1', 'SHMetric/Grammar/proba_0', 'SHMetric/Grammar/guess', 'SHMetric/Attribution/proba_1', 'SHMetric/Attribution/proba_0', 'SHMetric/Attribution/guess', 'SHMetric/Main ideas/proba_1', 'SHMetric/Main ideas/proba_0', 'SHMetric/Main ideas/guess', 'SHMetric/Conciseness/proba_1', 'SHMetric/Conciseness/proba_0', 'SHMetric/Conciseness/guess']\n",
            "⚡ Running Seahorse for Question 2...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_seahorse_metrics_samples.py\", line 5, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 405, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 216, in _lock_unlock_module\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "✅ After Question 2, Columns: ['id', 'Method', 'summary', 'text', 'SHMetric/Comprehensible/proba_1', 'SHMetric/Comprehensible/proba_0', 'SHMetric/Comprehensible/guess', 'SHMetric/Repetition/proba_1', 'SHMetric/Repetition/proba_0', 'SHMetric/Repetition/guess', 'SHMetric/Grammar/proba_1', 'SHMetric/Grammar/proba_0', 'SHMetric/Grammar/guess', 'SHMetric/Attribution/proba_1', 'SHMetric/Attribution/proba_0', 'SHMetric/Attribution/guess', 'SHMetric/Main ideas/proba_1', 'SHMetric/Main ideas/proba_0', 'SHMetric/Main ideas/guess', 'SHMetric/Conciseness/proba_1', 'SHMetric/Conciseness/proba_0', 'SHMetric/Conciseness/guess']\n",
            "⚡ Running Seahorse for Question 3...\n",
            "^C\n",
            "✅ After Question 3, Columns: ['id', 'Method', 'summary', 'text', 'SHMetric/Comprehensible/proba_1', 'SHMetric/Comprehensible/proba_0', 'SHMetric/Comprehensible/guess', 'SHMetric/Repetition/proba_1', 'SHMetric/Repetition/proba_0', 'SHMetric/Repetition/guess', 'SHMetric/Grammar/proba_1', 'SHMetric/Grammar/proba_0', 'SHMetric/Grammar/guess', 'SHMetric/Attribution/proba_1', 'SHMetric/Attribution/proba_0', 'SHMetric/Attribution/guess', 'SHMetric/Main ideas/proba_1', 'SHMetric/Main ideas/proba_0', 'SHMetric/Main ideas/guess', 'SHMetric/Conciseness/proba_1', 'SHMetric/Conciseness/proba_0', 'SHMetric/Conciseness/guess']\n",
            "⚡ Running Seahorse for Question 4...\n",
            "^C\n",
            "✅ After Question 4, Columns: ['id', 'Method', 'summary', 'text', 'SHMetric/Comprehensible/proba_1', 'SHMetric/Comprehensible/proba_0', 'SHMetric/Comprehensible/guess', 'SHMetric/Repetition/proba_1', 'SHMetric/Repetition/proba_0', 'SHMetric/Repetition/guess', 'SHMetric/Grammar/proba_1', 'SHMetric/Grammar/proba_0', 'SHMetric/Grammar/guess', 'SHMetric/Attribution/proba_1', 'SHMetric/Attribution/proba_0', 'SHMetric/Attribution/guess', 'SHMetric/Main ideas/proba_1', 'SHMetric/Main ideas/proba_0', 'SHMetric/Main ideas/guess', 'SHMetric/Conciseness/proba_1', 'SHMetric/Conciseness/proba_0', 'SHMetric/Conciseness/guess']\n",
            "⚡ Running Seahorse for Question 5...\n",
            "^C\n",
            "✅ After Question 5, Columns: ['id', 'Method', 'summary', 'text', 'SHMetric/Comprehensible/proba_1', 'SHMetric/Comprehensible/proba_0', 'SHMetric/Comprehensible/guess', 'SHMetric/Repetition/proba_1', 'SHMetric/Repetition/proba_0', 'SHMetric/Repetition/guess', 'SHMetric/Grammar/proba_1', 'SHMetric/Grammar/proba_0', 'SHMetric/Grammar/guess', 'SHMetric/Attribution/proba_1', 'SHMetric/Attribution/proba_0', 'SHMetric/Attribution/guess', 'SHMetric/Main ideas/proba_1', 'SHMetric/Main ideas/proba_0', 'SHMetric/Main ideas/guess', 'SHMetric/Conciseness/proba_1', 'SHMetric/Conciseness/proba_0', 'SHMetric/Conciseness/guess']\n",
            "⚡ Running Seahorse for Question 6...\n",
            "^C\n",
            "✅ After Question 6, Columns: ['id', 'Method', 'summary', 'text', 'SHMetric/Comprehensible/proba_1', 'SHMetric/Comprehensible/proba_0', 'SHMetric/Comprehensible/guess', 'SHMetric/Repetition/proba_1', 'SHMetric/Repetition/proba_0', 'SHMetric/Repetition/guess', 'SHMetric/Grammar/proba_1', 'SHMetric/Grammar/proba_0', 'SHMetric/Grammar/guess', 'SHMetric/Attribution/proba_1', 'SHMetric/Attribution/proba_0', 'SHMetric/Attribution/guess', 'SHMetric/Main ideas/proba_1', 'SHMetric/Main ideas/proba_0', 'SHMetric/Main ideas/guess', 'SHMetric/Conciseness/proba_1', 'SHMetric/Conciseness/proba_0', 'SHMetric/Conciseness/guess']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the speaker summaries file\n",
        "#df_seahorse = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/seahorse_speaker_summaries_2017.csv\")\n",
        "df_seahorse = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/results/2017/abstractive/seahorse_unique_summaries_2017.csv\")\n",
        "\n",
        "# Get SEAHORSE metrics (only the proba_1 columns)\n",
        "seahorse_cols = [col for col in df_seahorse.columns if col.startswith(\"SHMetric\") and col.endswith(\"proba_1\")]\n",
        "\n",
        "print(\"\\nSEAHORSE Metrics for GLIMPSE:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if seahorse_cols:\n",
        "    for col in seahorse_cols:\n",
        "        metric_name = col.split(\"/\")[1]\n",
        "        mean_score = df_seahorse[col].mean()\n",
        "        std_score = df_seahorse[col].std()\n",
        "        print(f\"{metric_name}: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "else:\n",
        "    print(\"No SEAHORSE metrics found in the file.\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Print some basic statistics\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(f\"Number of summaries: {len(df_seahorse)}\")\n",
        "print(f\"Average summary length: {df_seahorse['summary'].str.len().mean():.2f} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iZzZRg6Bgp9",
        "outputId": "fb48286b-ad7d-4cc2-9b11-eca646b78791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SEAHORSE Metrics for GLIMPSE:\n",
            "--------------------------------------------------\n",
            "Comprehensible: 0.92 ± 0.10\n",
            "Repetition: 0.97 ± 0.04\n",
            "Grammar: 0.77 ± 0.21\n",
            "Attribution: 0.08 ± 0.08\n",
            "Main ideas: 0.05 ± 0.10\n",
            "Conciseness: 0.06 ± 0.06\n",
            "--------------------------------------------------\n",
            "\n",
            "Basic Statistics:\n",
            "Number of summaries: 490\n",
            "Average summary length: 227.02 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extentions"
      ],
      "metadata": {
        "id": "fO1hXVIhXG1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extention1:PubMed Adaptation"
      ],
      "metadata": {
        "id": "_0C2CoryXHiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Load the data from CSV files:"
      ],
      "metadata": {
        "id": "2OdDb-4CXSIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QG0mpYlJXO1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load PubMed dataset (assuming CSV format)\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/pubmed_train.csv\")\n",
        "valid_df = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/pubmed_val.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/pubmed_test.csv\")\n",
        "\n",
        "# Check dataset structure\n",
        "print(train_df.columns)"
      ],
      "metadata": {
        "id": "hGot1CRBXTcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fc7ba7-3f4f-48c0-bb66-4343857d8bb2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['article', 'abstract', 'section_names'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values\n",
        "train_df = train_df.dropna(subset=['article', 'abstract'])\n",
        "valid_df = valid_df.dropna(subset=['article', 'abstract'])\n",
        "test_df = test_df.dropna(subset=['article', 'abstract'])"
      ],
      "metadata": {
        "id": "9pF4-sBIXTZ2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename necessary columns\n",
        "train_df.rename(columns={\"article\": \"text\", \"abstract\": \"summary\"}, inplace=True)\n",
        "valid_df.rename(columns={\"article\": \"text\", \"abstract\": \"summary\"}, inplace=True)\n",
        "test_df.rename(columns={\"article\": \"text\", \"abstract\": \"summary\"}, inplace=True)\n",
        "\n",
        "# Check dataset structure\n",
        "print(test_df.columns)"
      ],
      "metadata": {
        "id": "r-bNdBR7XTXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7111db-5180-4660-ead9-05a2b182dbb5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'summary', 'section_names'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save processed files\n",
        "train_df.to_csv(\"/content/drive/MyDrive/glimpse/pubmed/proccessed/processed_train.csv\", index=False)\n",
        "valid_df.to_csv(\"/content/drive/MyDrive/glimpse/pubmed/proccessed/processed_valid.csv\", index=False)\n",
        "test_df.to_csv(\"/content/drive/MyDrive/glimpse/pubmed/proccessed/processed_test.csv\", index=False)\n",
        "\n",
        "print(\"✅ PubMed dataset formatted successfully!\")"
      ],
      "metadata": {
        "id": "Ob0upAMqXTUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd340d1d-d82f-453d-8800-6e3383489f5a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PubMed dataset formatted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values per column\n",
        "print(train_df.isna().sum())\n",
        "print(valid_df.isna().sum())\n",
        "print(test_df.isna().sum())"
      ],
      "metadata": {
        "id": "5009xOqpXTRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26576727-0a4f-44f5-831f-3743afb1582c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text              0\n",
            "summary           0\n",
            "section_names    18\n",
            "dtype: int64\n",
            "text             0\n",
            "summary          0\n",
            "section_names    1\n",
            "dtype: int64\n",
            "text             0\n",
            "summary          0\n",
            "section_names    2\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df.shape)"
      ],
      "metadata": {
        "id": "qTKU4pkwXTOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8fab499-047a-4e73-b057-142a450e1d74"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6658, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Fine-Tune BART on PubMed"
      ],
      "metadata": {
        "id": "ZevE_9EwZ5TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed --quiet\n",
        "!pip install --upgrade transformers accelerate --no-cache-dir\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install -U bitsandbytes\n",
        "!pip uninstall bitsandbytes -y\n",
        "!pip install bitsandbytes\n",
        "!pip install nltk datasets pandas tqdm"
      ],
      "metadata": {
        "id": "fbYIANUeXTL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from datasets import Dataset\n",
        "\n",
        "# ✅ Free GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ✅ Load tokenizer & model\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# ✅ Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# ✅ Load training & validation datasets\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/proccessed/processed_train.csv\")\n",
        "valid_df = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/proccessed/processed_valid.csv\")\n",
        "\n",
        "# ✅ Reduce training dataset size to 25% (if needed for speed)\n",
        "train_df = train_df.sample(frac=0.25, random_state=42)\n",
        "\n",
        "# ✅ Convert to Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "valid_dataset = Dataset.from_pandas(valid_df)\n",
        "\n",
        "# ✅ Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"text\"], max_length=1024, truncation=True, padding=\"max_length\"\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples[\"summary\"], max_length=512, truncation=True, padding=\"max_length\"\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# ✅ Apply tokenization\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"summary\"])\n",
        "valid_dataset = valid_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"summary\"])\n",
        "\n",
        "print(\"✅ Tokenization completed!\")\n",
        "\n",
        "# ✅ Training Arguments (Lower Learning Rate for Stability)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/glimpse/pubmed/fine-tuned-bart\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=1,  # ✅ Adjust batch size (Reduce if OOM)\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=5,\n",
        "    save_total_limit=2,  # ✅ Keep only last 2 checkpoints\n",
        "    learning_rate=1e-5,  # 🔹 Reduce LR to avoid overfitting\n",
        "    fp16=True,  # ✅ Enable mixed precision for efficiency\n",
        "    logging_dir=\"/content/drive/MyDrive/glimpse/pubmed/logs\",\n",
        "    logging_steps=500,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# ✅ Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        ")\n",
        "\n",
        "# ✅ Train & Save After Each Epoch (Fixed Epoch Numbering)\n",
        "for epoch in range(1, 6):\n",
        "    print(f\"🚀 Starting Epoch {epoch}/5...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # ✅ Convert model back to FP32 before saving (Fix FP16 issues)\n",
        "    model = model.float()\n",
        "\n",
        "    save_dir = f\"/content/drive/MyDrive/glimpse/pubmed/fine-tuned-bart/epoch-{epoch}\"\n",
        "    model.save_pretrained(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"✅ Model saved at {save_dir}\")\n",
        "\n",
        "# ✅ Save Final Model in FP32\n",
        "model.save_pretrained(\"/content/drive/MyDrive/glimpse/pubmed/fine-tuned-bart\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/glimpse/pubme/fine-tuned-bart-final\")\n",
        "\n",
        "print(\"✅ Fine-tuning completed and final model saved in FP32!\")"
      ],
      "metadata": {
        "id": "BNjWBqiNXTI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generate Summaries Using Fine-Tuned Model"
      ],
      "metadata": {
        "id": "3QeL46FqaO-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset and check if it's properly loaded\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/proccessed/processed_test.csv\")\n",
        "print(df_test.columns)  # Should show: ['text', 'gold', 'section_names']\n",
        "print(df_test.isnull().sum())  # To check if there are any NaN values\n",
        "print(df_test.head())  # To see the first few rows\n",
        "print(df_test.shape)"
      ],
      "metadata": {
        "id": "OOwUeCUrXTGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c189bb5-a96d-4c10-e6fd-b003260f5c79"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'summary', 'section_names'], dtype='object')\n",
            "text             0\n",
            "summary          0\n",
            "section_names    2\n",
            "dtype: int64\n",
            "                                                text  \\\n",
            "0  anxiety affects quality of life in those livin...   \n",
            "1  small non - coding rnas are transcribed into m...   \n",
            "2  ohss is a serious complication of ovulation in...   \n",
            "3  congenital adrenal hyperplasia ( cah ) refers ...   \n",
            "4  type 1 diabetes ( t1d ) results from the destr...   \n",
            "\n",
            "                                             summary  \\\n",
            "0   research on the implications of anxiety in pa...   \n",
            "1   small non - coding rnas include sirna , mirna...   \n",
            "2   objective : to evaluate the efficacy and safe...   \n",
            "3   congenital adrenal hyperplasia is a group of ...   \n",
            "4   objective(s):pentoxifylline is an immunomodul...   \n",
            "\n",
            "                                       section_names  \n",
            "0  1. Introduction\\n2. Methods\\n3. Results\\n4. Di...  \n",
            "1  Introduction\\nAberrant Expression of miRNA in ...  \n",
            "2  Introduction\\nMaterials and Methods\\nResults\\n...  \n",
            "3                                         I\\nM\\nR\\nD  \n",
            "4  Introduction\\nMaterials and Methods\\nDrug and ...  \n",
            "(6658, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Abstractive candidates"
      ],
      "metadata": {
        "id": "SXrig9KWargy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ✅ Load Fine-Tuned Model & Tokenizer\n",
        "model_path = \"/content/drive/MyDrive/glimpse/pubmed/fine-tuned-bart-final\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# ✅ Fix Missing Token Issues (Ensure correct settings)\n",
        "model.config.forced_bos_token_id = tokenizer.bos_token_id\n",
        "model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# ✅ Load Test Dataset\n",
        "test_df_path = \"/content/drive/MyDrive/glimpse/pubmed/proccessed/processed_test.csv\"\n",
        "test_df = pd.read_csv(test_df_path)\n",
        "\n",
        "# ✅ Remove Null Rows (if any)\n",
        "test_df.dropna(subset=[\"text\"], inplace=True)\n",
        "\n",
        "# ✅ Function to Generate Summaries\n",
        "def generate_summary(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024, padding=\"max_length\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for speedup\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3,  # Prevent repetition\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# ✅ Generate Summaries for the Entire Dataset\n",
        "summaries = []\n",
        "for text in tqdm(test_df[\"text\"], desc=\"Generating Summaries\"):\n",
        "    summaries.append(generate_summary(text))\n",
        "\n",
        "# ✅ Add Summaries to the DataFrame\n",
        "test_df[\"summary\"] = summaries\n",
        "\n",
        "# ✅ Reset index to create 'index' column\n",
        "test_df = test_df.reset_index()\n",
        "\n",
        "# ✅ Add id_candidate column (same as index in this case since we're generating one summary per text)\n",
        "test_df['id_candidate'] = test_df['index']\n",
        "\n",
        "# ✅ Create output directory and generate filename with timestamp\n",
        "output_dir = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "output_file = os.path.join(output_dir, f\"abstractive_summaries_{timestamp}.csv\")\n",
        "\n",
        "# ✅ Save the Results\n",
        "test_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"✅ Summaries saved to: {output_file}\")\n",
        "print(f\"Columns in saved file: {test_df.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "jEYjFoGKZ6us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/glimpse/glimpse-mds')\n",
        "from rsasumm.rsa_reranker import RSAReranking\n",
        "print(\"Import successful\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJhUkkYZZ6r3",
        "outputId": "abb5a290-aec6-41bd-d17c-f56fba25cccb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/abstractive_summaries_2025-04-02-02-13-59.csv\"\n",
        "df= pd.read_csv(file_path)\n",
        "\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W7PJk-fZ6pT",
        "outputId": "cce2cf0b-713a-4f96-d7a5-9751daba2c89"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['index', 'text', 'gold', 'section_names', 'summary', 'id_candidate',\n",
            "       'id'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def add_id_column_if_missing(file_path):\n",
        "    \"\"\"\n",
        "    Check if CSV file has 'id' column, if not add it and update the file\n",
        "    Args:\n",
        "        file_path: Path to the CSV file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV file\n",
        "        print(f\"📄 Reading file: {file_path}\")\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Print current columns\n",
        "        print(f\"Current columns: {df.columns.tolist()}\")\n",
        "\n",
        "        # Check if 'id' column exists\n",
        "        if 'id' not in df.columns:\n",
        "            print(\"'id' column not found. Adding it...\")\n",
        "            # Add 'id' column (using index + 1 to start from 1)\n",
        "            df['id'] = df.index + 1\n",
        "\n",
        "            # Save back to the same file\n",
        "            df.to_csv(file_path, index=False)\n",
        "            print(f\"✅ Added 'id' column and updated file: {file_path}\")\n",
        "            print(f\"New columns: {df.columns.tolist()}\")\n",
        "        else:\n",
        "            print(\"'id' column already exists. No changes needed.\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/abstractive_summaries_2025-04-02-02-13-59.csv\"\n",
        "df = add_id_column_if_missing(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lmqdozYZ6mp",
        "outputId": "fcbf5bfb-6862-45be-ecc3-87f28c9c2cda"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Reading file: /content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/abstractive_summaries_2025-04-02-02-13-59.csv\n",
            "Current columns: ['index', 'text', 'gold', 'section_names', 'summary', 'id_candidate', 'id']\n",
            "'id' column already exists. No changes needed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read and display the first few rows of your CSV file\n",
        "file_path = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/abstractive_summaries_2025-04-02-02-13-59.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"First few rows of the CSV file:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataFrame info:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlrG_KeHZ6kF",
        "outputId": "98ff9b5b-3fc3-48e7-e0af-0ad04ac26373"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the CSV file:\n",
            "   index                                               text  \\\n",
            "0      0  anxiety affects quality of life in those livin...   \n",
            "1      1  small non - coding rnas are transcribed into m...   \n",
            "2      2  ohss is a serious complication of ovulation in...   \n",
            "3      3  congenital adrenal hyperplasia ( cah ) refers ...   \n",
            "4      4  type 1 diabetes ( t1d ) results from the destr...   \n",
            "\n",
            "                                                gold  \\\n",
            "0   research on the implications of anxiety in pa...   \n",
            "1   small non - coding rnas include sirna , mirna...   \n",
            "2   objective : to evaluate the efficacy and safe...   \n",
            "3   congenital adrenal hyperplasia is a group of ...   \n",
            "4   objective(s):pentoxifylline is an immunomodul...   \n",
            "\n",
            "                                       section_names  \\\n",
            "0  1. Introduction\\n2. Methods\\n3. Results\\n4. Di...   \n",
            "1  Introduction\\nAberrant Expression of miRNA in ...   \n",
            "2  Introduction\\nMaterials and Methods\\nResults\\n...   \n",
            "3                                         I\\nM\\nR\\nD   \n",
            "4  Introduction\\nMaterials and Methods\\nDrug and ...   \n",
            "\n",
            "                                             summary  id_candidate  id  \n",
            "0   \\n introduction . \\n anxiety is a prevalent m...             0   1  \n",
            "1   micrornas ( mirnas ) are small non - coding r...             1   2  \n",
            "2   ovarian hyperstimulation syndrome ( ohss ) is...             2   3  \n",
            "3   congenital adrenal hyperplasia ( cah ) refers...             3   4  \n",
            "4   objective(s):type 1 diabetes ( t1d ) results ...             4   5  \n",
            "\n",
            "DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6658 entries, 0 to 6657\n",
            "Data columns (total 7 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   index          6658 non-null   int64 \n",
            " 1   text           6658 non-null   object\n",
            " 2   gold           6658 non-null   object\n",
            " 3   section_names  6656 non-null   object\n",
            " 4   summary        6658 non-null   object\n",
            " 5   id_candidate   6658 non-null   int64 \n",
            " 6   id             6658 non-null   int64 \n",
            "dtypes: int64(3), object(4)\n",
            "memory usage: 364.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1 python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/src/compute_rsa.py \\\n",
        "            --model_name facebook/bart-large-cnn \\\n",
        "            --summaries /content/drive/MyDrive/glimpse/pubmed/extractive_summaries/extractive_sentences-_-processed_test-_-none-_-2025-03-27-11-58-15.csv \\\n",
        "            --output_dir /content/drive/MyDrive/glimpse/pubmed/extractive_summaries \\\n",
        "            --device cuda"
      ],
      "metadata": {
        "id": "ZhJc21U3Z6hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_glimpse_summaries_from_rsa.py \\\n",
        "    --rsa_scores \"/content/drive/MyDrive/glimpse/pubmed/rsa_scores/abstractive_summaries_2025-04-02-02-13-59-_-r3-_-rsa_reranked-facebook-bart-large-cnn.pk\"\\\n",
        "    --output_dir \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries\" \\\n",
        "    --year \"pubmed\" \\\n",
        "    --n_unique_sentences 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT7uyUZOZ6e1",
        "outputId": "a2244f7e-a7b9-48b3-b099-cbdb9791436e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading RSA scores from /content/drive/MyDrive/glimpse/pubmed/rsa_scores/abstractive_summaries_2025-04-02-02-13-59-_-r3-_-rsa_reranked-facebook-bart-large-cnn.pk...\n",
            "\n",
            "Generating summaries for 6658 documents...\n",
            "100% 6658/6658 [00:01<00:00, 3367.48it/s]\n",
            "\n",
            "Summary Statistics:\n",
            "GLIMPSE-Speaker summaries: 6658\n",
            "GLIMPSE-Unique summaries: 6658\n",
            "Average speaker summary length: 221.00 chars\n",
            "Average unique summary length: 220.27 chars\n",
            "\n",
            "Saved summaries to:\n",
            "- /content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/speaker_summaries_pubmed.csv\n",
            "- /content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/unique_summaries_pubmed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the generated files\n",
        "abstractive_pubmed_df = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/rough_unique_summaries_pubmed.csv\")\n",
        "\n",
        "print(\"abstractive pubmed summaries columns:\", abstractive_pubmed_df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hyfoBmbZ6cQ",
        "outputId": "bef6ab66-a9c1-4de8-e2c9-d370021f6d3c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abstractive pubmed summaries columns: ['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2', 'common/rougeL', 'common/rougeLsum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "speaker_path = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/rough_speaker_summaries_pubmed.csv\"\n",
        "unique_path = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/rough_unique_summaries_pubmed.csv\"\n",
        "\n",
        "# Read and modify speaker summaries\n",
        "speaker_df = pd.read_csv(speaker_path)\n",
        "speaker_df = speaker_df.rename(columns={'text': 'gold'})\n",
        "speaker_df.to_csv(speaker_path, index=False)\n",
        "\n",
        "# Read and modify unique summaries\n",
        "unique_df = pd.read_csv(unique_path)\n",
        "unique_df = unique_df.rename(columns={'text': 'gold'})\n",
        "unique_df.to_csv(unique_path, index=False)\n",
        "\n",
        "# Verify the changes\n",
        "print(\"Speaker summaries columns:\", speaker_df.columns.tolist())\n",
        "print(\"Unique summaries columns:\", unique_df.columns.tolist())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/rough_speaker_summaries_pubmed.csv\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/rough_unique_summaries_pubmed.csv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Paths to the updated CSVs after running ROUGE evaluation\n",
        "output_speaker = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/rough_speaker_summaries_pubmed.csv\"\n",
        "output_unique = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/rough_unique_summaries_pubmed.csv\"\n",
        "\n",
        "# Load CSV files\n",
        "df_speaker = pd.read_csv(output_speaker)\n",
        "df_unique = pd.read_csv(output_unique)\n",
        "\n",
        "# Check if ROUGE scores exist\n",
        "print(\"Columns in GLIMPSE-Speaker CSV:\", df_speaker.columns)\n",
        "print(\"Columns in GLIMPSE-Unique CSV:\", df_unique.columns)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Select only ROUGE score columns\n",
        "rouge_columns = [\"common/rouge1\", \"common/rouge2\", \"common/rougeL\", \"common/rougeLsum\"]\n",
        "\n",
        "# Compute mean and std ROUGE scores (as decimals, not percentages)\n",
        "mean_speaker = df_speaker[rouge_columns].mean()  # Removed * 100\n",
        "std_speaker = df_speaker[rouge_columns].std()    # Removed * 100\n",
        "mean_unique = df_unique[rouge_columns].mean()    # Removed * 100\n",
        "std_unique = df_unique[rouge_columns].std()      # Removed * 100\n",
        "\n",
        "# Create a formatted table\n",
        "print(\"\\nROUGE Scores:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<12} {'GLIMPSE-Speaker':>22} {'GLIMPSE-Unique':>22}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "metrics_display = {\n",
        "    \"common/rouge1\": \"ROUGE-1\",\n",
        "    \"common/rouge2\": \"ROUGE-2\",\n",
        "    \"common/rougeL\": \"ROUGE-L\",\n",
        "    \"common/rougeLsum\": \"ROUGE-Lsum\"\n",
        "}\n",
        "\n",
        "for col in rouge_columns:\n",
        "    metric_name = metrics_display[col]\n",
        "    # Format scores to match paper style (0.XX ±0.XX)\n",
        "    speaker_score = f\"{mean_speaker[col]:.2f} ±{std_speaker[col]:.2f}\"\n",
        "    unique_score = f\"{mean_unique[col]:.2f} ±{std_unique[col]:.2f}\"\n",
        "    print(f\"{metric_name:<12} {speaker_score:>22} {unique_score:>22}\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "fgKGy2WsZ6Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to the CSV file\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/seahorse_speaker_summaries_pubmed.csv\"\n",
        "file_path = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/seahorse_unique_summaries_pubmed.csv\"\n",
        "\n",
        "for question in range(1, 7):\n",
        "    print(f\"⚡ Running Seahorse for Question {question}...\")\n",
        "\n",
        "    # Reload dataset before each run to avoid missing columns\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        print(\"❌ ERROR: 'text' and 'summary' columns missing before running Seahorse. Stopping execution.\")\n",
        "        break  # Stop if important columns are missing\n",
        "\n",
        "    # Run the Seahorse evaluation script\n",
        "    !python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/Copyof_evaluate_seahorse_metrics_samples.py \\\n",
        "        --summaries {file_path} \\\n",
        "        --question {question} \\\n",
        "        --batch_size 8 \\\n",
        "        --device cuda\n",
        "\n",
        "\n",
        "    # ✅ Verify columns after running Seahorse\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"✅ After Question {question}, Columns:\", df.columns.tolist())\n",
        "\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        print(\"❌ ERROR: 'text' and 'summary' columns missing after running Seahorse. Stopping execution.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "PREIgVCGZ6XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to the CSV file\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/seahorse_speaker_summaries_pubmed.csv\"\n",
        "file_path = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/seahorse_unique_summaries_pubmed.csv\"\n",
        "\n",
        "# Read the CSV file\n",
        "df_seahorse = pd.read_csv(file_path)\n",
        "\n",
        "# Get SEAHORSE metrics (only the proba_1 columns)\n",
        "seahorse_cols = [col for col in df_seahorse.columns if col.startswith(\"SHMetric\") and col.endswith(\"proba_1\")]\n",
        "\n",
        "print(\"\\nSEAHORSE Metrics for GLIMPSE:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if seahorse_cols:\n",
        "    for col in seahorse_cols:\n",
        "        metric_name = col.split(\"/\")[1]\n",
        "        mean_score = df_seahorse[col].mean()\n",
        "        std_score = df_seahorse[col].std()\n",
        "        print(f\"{metric_name}: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "else:\n",
        "    print(\"No SEAHORSE metrics found in the file.\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Print some basic statistics\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(f\"Number of summaries: {len(df_seahorse)}\")\n",
        "print(f\"Average summary length: {df_seahorse['summary'].str.len().mean():.2f} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuBHpGDhZ6Rj",
        "outputId": "be905b76-b023-4c33-9c0e-816fb134cfd2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SEAHORSE Metrics for GLIMPSE:\n",
            "--------------------------------------------------\n",
            "Comprehensible: 0.79 ± 0.19\n",
            "Repetition: 0.93 ± 0.13\n",
            "Grammar: 0.51 ± 0.24\n",
            "Attribution: 0.41 ± 0.10\n",
            "Main ideas: 0.15 ± 0.09\n",
            "Conciseness: 0.18 ± 0.07\n",
            "--------------------------------------------------\n",
            "\n",
            "Basic Statistics:\n",
            "Number of summaries: 6658\n",
            "Average summary length: 220.27 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Extractive candidates"
      ],
      "metadata": {
        "id": "u4dkCc8Yk57t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the base paths for PubMed processed data and output\n",
        "processed_dir = \"/content/drive/MyDrive/glimpse/pubmed/proccessed\"\n",
        "output_dir = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries\"\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Path to the PubMed test dataset\n",
        "dataset_path = f\"{processed_dir}/processed_test.csv\"\n",
        "\n",
        "# Check if input file exists\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(f\"Error: Input file {dataset_path} does not exist.\")\n",
        "else:\n",
        "    print(f\"Processing PubMed extractive summarization...\")\n",
        "\n",
        "    # Run the extractive summarization script\n",
        "    exit_code = os.system(f\"\"\"\n",
        "    python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/data_loading/generate_extractive_candidates.py \\\n",
        "        --dataset_path {dataset_path} \\\n",
        "        --output_dir {output_dir}\n",
        "    \"\"\")\n",
        "\n",
        "    if exit_code != 0:\n",
        "        print(f\"❌ Error: Script failed with exit code {exit_code}\")\n",
        "    else:\n",
        "        print(f\"✅ Extractive summaries for PubMed generated successfully!\")\n",
        "\n",
        "print(\"🎯 Task completed!\")\n"
      ],
      "metadata": {
        "id": "n9k83uDQZ6O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/extractive_sentences.csv\")"
      ],
      "metadata": {
        "id": "-jzUHPpZZ6L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "print(df.head())\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "MrQDe2zmlGLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: (Optional) Add id Column (if not present)\n",
        "import pandas as pd\n",
        "\n",
        "# Load your extractive summary file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/extractive_sentences-_-processed_test-_-none-_-2025-03-27-11-58-15.csv\")\n",
        "\n",
        "# Add 'id' column\n",
        "df[\"id\"] = df[\"index\"]\n",
        "\n",
        "# Save it\n",
        "df.to_csv(\"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/extractive_sentences.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "aHqOjzYUlGIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your full extractive summaries dataset\n",
        "input_path = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/extractive_sentences.csv\"\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "# Sample 1000 unique IDs\n",
        "sampled_ids = df['id'].drop_duplicates().sample(n=1000, random_state=42)\n",
        "\n",
        "# Select only rows corresponding to those sampled IDs\n",
        "df_small = df[df['id'].isin(sampled_ids)].reset_index(drop=True)\n",
        "\n",
        "print(f\"New sampled dataset shape: {df_small.shape}\")\n",
        "\n",
        "# Save the smaller dataset\n",
        "output_path = \"/content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/sample_1000_ids.csv\"\n",
        "df_small.to_csv(output_path, index=False)\n",
        "print(f\"✅ Smaller dataset saved at {output_path}\")"
      ],
      "metadata": {
        "id": "DZ-PIvGylGFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Run RSA Reranking\n",
        "\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/src/compute_rsa.py \\\n",
        "  --model_name facebook/bart-large-cnn \\\n",
        "  --summaries /content/drive/MyDrive/glimpse/pubmed/abstractive_summaries/sample_1000_ids.csv \\\n",
        "  --output_dir /content/drive/MyDrive/glimpse/pubmed/extractive_summaries/ \\\n",
        "  --device cuda"
      ],
      "metadata": {
        "id": "b-j224qYlGCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating glimpse_speaker and glimpse_unique with cleaning\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "pk_dir_path = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries\"\n",
        "output_speaker = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/pubmed_extractive_speaker.csv\"\n",
        "output_unique = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/pubmed_extractive_unique.csv\"\n",
        "\n",
        "# ✅ Function to clean text\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"\\n\", \" \").strip()\n",
        "    return \"\"\n",
        "\n",
        "# Lists to hold extracted data\n",
        "rows_speaker = []\n",
        "rows_unique = []\n",
        "\n",
        "# Process each .pk file\n",
        "for pk_file in Path(pk_dir_path).glob(\"*.pk\"):\n",
        "    print(f\"Processing file: {pk_file}\")\n",
        "    try:\n",
        "        # Load .pk file\n",
        "        with open(pk_file, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        # Extract \"results\" section\n",
        "        results = data.get(\"results\", [])\n",
        "\n",
        "        for result in results:\n",
        "            id_ = result[\"id\"][0]  # Extract URL from tuple\n",
        "            gold_summary = clean_text(result[\"gold\"])  # ✅ Clean gold reference summary\n",
        "\n",
        "            # ✅ Extract GLIMPSE-Speaker summaries (best_rsa)\n",
        "            best_rsa = result.get(\"best_rsa\", [])\n",
        "            if isinstance(best_rsa, (np.ndarray, list)) and len(best_rsa) > 0:\n",
        "                for summary in best_rsa:\n",
        "                    cleaned_summary = clean_text(summary)\n",
        "                    if len(cleaned_summary.split()) > 5:  # ✅ Remove very short summaries\n",
        "                        rows_speaker.append({\n",
        "                            \"id\": id_,\n",
        "                            \"Method\": \"GLIMPSE-Speaker\",\n",
        "                            \"summary\": cleaned_summary,\n",
        "                            \"gold\": gold_summary\n",
        "                        })\n",
        "\n",
        "            # ✅ Extract GLIMPSE-Unique summaries (Top 3 based on consensuality score)\n",
        "            initial_consensuality_scores = result.get(\"initial_consensuality_scores\", None)\n",
        "            if isinstance(initial_consensuality_scores, pd.Series) and not initial_consensuality_scores.empty:\n",
        "                sorted_summaries = initial_consensuality_scores.sort_values(ascending=False).index[:3]\n",
        "                for summary in sorted_summaries:\n",
        "                    cleaned_summary = clean_text(summary)\n",
        "                    if len(cleaned_summary.split()) > 5:  # ✅ Remove very short summaries\n",
        "                        rows_unique.append({\n",
        "                            \"id\": id_,\n",
        "                            \"Method\": \"GLIMPSE-Unique\",\n",
        "                            \"summary\": cleaned_summary,\n",
        "                            \"gold\": gold_summary\n",
        "                        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing {pk_file}: {e}\")\n",
        "\n",
        "# Convert lists to DataFrames\n",
        "df_speaker = pd.DataFrame(rows_speaker)\n",
        "df_unique = pd.DataFrame(rows_unique)\n",
        "\n",
        "# Save extracted and cleaned data to CSV files\n",
        "df_speaker.to_csv(output_speaker, index=False)\n",
        "df_unique.to_csv(output_unique, index=False)\n",
        "\n",
        "print(\"✅ GLIMPSE-Speaker summaries saved to:\", output_speaker)\n",
        "print(\"✅ GLIMPSE-Unique summaries saved to:\", output_unique)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng7B5ZFVlF_x",
        "outputId": "bdd8f26d-4c87-483e-810e-fa270ed8b200"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: /content/drive/MyDrive/glimpse/pubmed/extractive_summaries/sample_1000_ids-_-r3-_-rsa_reranked-facebook-bart-large-cnn.pk\n",
            "✅ GLIMPSE-Speaker summaries saved to: /content/drive/MyDrive/glimpse/pubmed/extractive_summaries/pubmed_extractive_speaker.csv\n",
            "✅ GLIMPSE-Unique summaries saved to: /content/drive/MyDrive/glimpse/pubmed/extractive_summaries/pubmed_extractive_unique.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Evaluate with ROUGE\n",
        "#!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/pubmed/extractive_summaries/rough_pubmed_extractive_speaker.csv\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/pubmed/extractive_summaries/rough_pubmed_extractive_unique.csv\n"
      ],
      "metadata": {
        "id": "xg-tjmNWlF8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the updated CSVs after running ROUGE evaluation\n",
        "output_speaker = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/rough_pubmed_extractive_speaker.csv\"\n",
        "output_unique = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/rough_pubmed_extractive_unique.csv\"\n",
        "\n",
        "# Load CSV files\n",
        "df_speaker = pd.read_csv(output_speaker)\n",
        "df_unique = pd.read_csv(output_unique)\n",
        "\n",
        "# Check if ROUGE scores exist\n",
        "print(\"Columns in GLIMPSE-Speaker CSV:\", df_speaker.columns)\n",
        "print(\"Columns in GLIMPSE-Unique CSV:\", df_unique.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeF9oGDFlF5h",
        "outputId": "4e5e75b5-63ac-4061-804a-554e151f1bd4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in GLIMPSE-Speaker CSV: Index(['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2',\n",
            "       'common/rougeL', 'common/rougeLsum'],\n",
            "      dtype='object')\n",
            "Columns in GLIMPSE-Unique CSV: Index(['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2',\n",
            "       'common/rougeL', 'common/rougeLsum'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only ROUGE score columns\n",
        "rouge_columns = [\"common/rouge1\", \"common/rouge2\", \"common/rougeL\", \"common/rougeLsum\"]\n",
        "\n",
        "# Compute mean and std ROUGE scores (as decimals, not percentages)\n",
        "mean_speaker = df_speaker[rouge_columns].mean()  # Removed * 100\n",
        "std_speaker = df_speaker[rouge_columns].std()    # Removed * 100\n",
        "mean_unique = df_unique[rouge_columns].mean()    # Removed * 100\n",
        "std_unique = df_unique[rouge_columns].std()      # Removed * 100\n",
        "\n",
        "# Create a formatted table\n",
        "print(\"\\nROUGE Scores:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<12} {'GLIMPSE-Speaker':>22} {'GLIMPSE-Unique':>22}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "metrics_display = {\n",
        "    \"common/rouge1\": \"ROUGE-1\",\n",
        "    \"common/rouge2\": \"ROUGE-2\",\n",
        "    \"common/rougeL\": \"ROUGE-L\",\n",
        "    \"common/rougeLsum\": \"ROUGE-Lsum\"\n",
        "}\n",
        "\n",
        "for col in rouge_columns:\n",
        "    metric_name = metrics_display[col]\n",
        "    # Format scores to match paper style (0.XX ±0.XX)\n",
        "    speaker_score = f\"{mean_speaker[col]:.2f} ±{std_speaker[col]:.2f}\"\n",
        "    unique_score = f\"{mean_unique[col]:.2f} ±{std_unique[col]:.2f}\"\n",
        "    print(f\"{metric_name:<12} {speaker_score:>22} {unique_score:>22}\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-YJLxn1lF2h",
        "outputId": "30a027a0-a77c-479b-cac1-06e99b075207"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores:\n",
            "============================================================\n",
            "Metric              GLIMPSE-Speaker         GLIMPSE-Unique\n",
            "------------------------------------------------------------\n",
            "ROUGE-1                  0.14 ±0.08             0.13 ±0.07\n",
            "ROUGE-2                  0.05 ±0.06             0.04 ±0.05\n",
            "ROUGE-L                  0.10 ±0.06             0.09 ±0.05\n",
            "ROUGE-Lsum               0.10 ±0.06             0.09 ±0.05\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing df s to for seahorse calculation\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load CSV\n",
        "file_path = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/seahorse_pubmed_extractive_speaker.csv\"\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/seahorse_pubmed_extractive_unique.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename 'gold' → 'text'\n",
        "df.rename(columns={\"gold\": \"text\"}, inplace=True)\n",
        "\n",
        "# Save the updated file\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "print(\"✅ CSV column renamed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rda7P1y6lFzh",
        "outputId": "0d2ddd93-797b-4dcc-c468-be1c925b632c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CSV column renamed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Evaluate with SEAHORSE (Human-like metrics)\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the CSV file\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/seahorse_pubmed_extractive_speaker.csv\"\n",
        "file_path = \"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/seahorse_pubmed_extractive_unique.csv\"\n",
        "\n",
        "for question in range(1, 7):\n",
        "    print(f\"⚡ Running Seahorse for Question {question}...\")\n",
        "\n",
        "    # Reload dataset before each run to avoid missing columns\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        print(\"❌ ERROR: 'text' and 'summary' columns missing before running Seahorse. Stopping execution.\")\n",
        "        break  # Stop if important columns are missing\n",
        "\n",
        "    # Run the Seahorse evaluation script\n",
        "    !python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/Copyof_evaluate_seahorse_metrics_samples.py \\\n",
        "        --summaries {file_path} \\\n",
        "        --question {question} \\\n",
        "        --batch_size 8 \\\n",
        "        --device cuda\n",
        "\n",
        "    # ✅ Verify columns after running Seahorse\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"✅ After Question {question}, Columns:\", df.columns.tolist())\n",
        "\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        print(\"❌ ERROR: 'text' and 'summary' columns missing after running Seahorse. Stopping execution.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "yiVetWJnoBY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the speaker summaries file\n",
        "#df_seahorse = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/seahorse_pubmed_extractive_speaker.csv\")\n",
        "df_seahorse = pd.read_csv(\"/content/drive/MyDrive/glimpse/pubmed/extractive_summaries/seahorse_pubmed_extractive_unique.csv\")\n",
        "\n",
        "# Get SEAHORSE metrics (only the proba_1 columns)\n",
        "seahorse_cols = [col for col in df_seahorse.columns if col.startswith(\"SHMetric\") and col.endswith(\"proba_1\")]\n",
        "\n",
        "print(\"\\nSEAHORSE Metrics for GLIMPSE:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if seahorse_cols:\n",
        "    for col in seahorse_cols:\n",
        "        metric_name = col.split(\"/\")[1]\n",
        "        mean_score = df_seahorse[col].mean()\n",
        "        std_score = df_seahorse[col].std()\n",
        "        print(f\"{metric_name}: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "else:\n",
        "    print(\"No SEAHORSE metrics found in the file.\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Print some basic statistics\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(f\"Number of summaries: {len(df_seahorse)}\")\n",
        "print(f\"Average summary length: {df_seahorse['summary'].str.len().mean():.2f} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubot-3WwoBWK",
        "outputId": "831a2f5b-a899-4095-d511-e13e475f8806"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SEAHORSE Metrics for GLIMPSE:\n",
            "--------------------------------------------------\n",
            "Comprehensible: 0.81 ± 0.18\n",
            "Repetition: 0.95 ± 0.10\n",
            "Grammar: 0.53 ± 0.22\n",
            "Attribution: 0.38 ± 0.07\n",
            "Main ideas: 0.08 ± 0.08\n",
            "Conciseness: 0.14 ± 0.05\n",
            "--------------------------------------------------\n",
            "\n",
            "Basic Statistics:\n",
            "Number of summaries: 2969\n",
            "Average summary length: 180.29 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## extension2: Model Comparison"
      ],
      "metadata": {
        "id": "IxJJQFlToWVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
        "print(f\"GPU Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU detected'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG2PU7TmoBUB",
        "outputId": "cac192ff-0b37-4f8c-fa45-83a457a42ab7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available? False\n",
            "GPU Device: No GPU detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "torch.backends.cudnn.benchmark = True  # Optimized GPU usage\n",
        "transformers.utils.logging.set_verbosity_info()"
      ],
      "metadata": {
        "id": "9NpOzC4AoBRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# List of models to run\n",
        "models = [\"google/pegasus-large\",\n",
        "          #\"t5-large\"\n",
        "           ]\n",
        "\n",
        "# Define the output directory\n",
        "output_dir = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2\"\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for model in models:\n",
        "    print(f\"⚡ Running summarization with {model}...\")\n",
        "\n",
        "#/content/drive/MyDrive/glimpse/glimpse-mds/glimpse/data_loading/pegasus_generate_abstractive_candidates.py\n",
        "#/content/drive/MyDrive/glimpse/glimpse-mds/glimpse/data_loading/T5_generate_abstractive_candidates.py\n",
        "    command = f\"\"\"\n",
        "    python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/data_loading/pegasus_generate_abstractive_candidates.py \\\n",
        "        --model_name {model} \\\n",
        "        --dataset_path /content/drive/MyDrive/glimpse/glimpse-mds/data/processed/all_reviews_2017.csv \\\n",
        "        --decoding_config beam_search \\\n",
        "        --batch_size 8 \\\n",
        "        --device cuda \\\n",
        "        --output_dir \"{output_dir}\"\n",
        "    \"\"\"\n",
        "\n",
        "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "\n",
        "    print(stdout.decode())  # ✅ See if script is running\n",
        "    print(stderr.decode())  # ✅ See errors if script is failing\n"
      ],
      "metadata": {
        "id": "wER-wJRToBOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/glimpse/glimpse-mds/')\n",
        "from rsasumm.rsa_reranker import RSAReranking\n",
        "print(\"Import successful\")"
      ],
      "metadata": {
        "id": "AIiLHUa-oBMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute rsa score for t5 and pegasus summeries\n",
        "# /content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/t5-large/t5-large-_-all_reviews_2017-_-beam_search-_-trimmed-_-2025-03-27-08-36-37.csv\n",
        "# /content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/google_pegasus-large/google_pegasus-large-_-all_reviews_2017-_-beam_search-_-trimmed-_-2025-03-27-09-05-28.csv\n",
        "\n",
        "!CUDA_LAUNCH_BLOCKING=1 python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/src/compute_rsa.py \\\n",
        "            --model_name facebook/bart-large-cnn \\\n",
        "            --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/google_pegasus-large/google_pegasus-large-_-all_reviews_2017-_-beam_search-_-trimmed-_-2025-03-27-09-05-28.csv \\\n",
        "            --output_dir /content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores  \\\n",
        "            --device cuda"
      ],
      "metadata": {
        "id": "JGZhAe4loBJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating glimpse_speaker and glimpse_unique with cleaning\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "pk_dir_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus\"\n",
        "output_speaker = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/pegasus_abstractive_speaker.csv\"\n",
        "output_unique = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/pegasus_abstractive_unique.csv\"\n",
        "\n",
        "# ✅ Function to clean text\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"\\n\", \" \").strip()\n",
        "    return \"\"\n",
        "\n",
        "# Lists to hold extracted data\n",
        "rows_speaker = []\n",
        "rows_unique = []\n",
        "\n",
        "# Process each .pk file\n",
        "for pk_file in Path(pk_dir_path).glob(\"*.pk\"):\n",
        "    print(f\"Processing file: {pk_file}\")\n",
        "    try:\n",
        "        # Load .pk file\n",
        "        with open(pk_file, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        # Extract \"results\" section\n",
        "        results = data.get(\"results\", [])\n",
        "\n",
        "        for result in results:\n",
        "            id_ = result[\"id\"][0]  # Extract URL from tuple\n",
        "            gold_summary = clean_text(result[\"gold\"])  # ✅ Clean gold reference summary\n",
        "\n",
        "            # ✅ Extract GLIMPSE-Speaker summaries (best_rsa)\n",
        "            best_rsa = result.get(\"best_rsa\", [])\n",
        "            if isinstance(best_rsa, (np.ndarray, list)) and len(best_rsa) > 0:\n",
        "                for summary in best_rsa:\n",
        "                    cleaned_summary = clean_text(summary)\n",
        "                    if len(cleaned_summary.split()) > 5:  # ✅ Remove very short summaries\n",
        "                        rows_speaker.append({\n",
        "                            \"id\": id_,\n",
        "                            \"Method\": \"GLIMPSE-Speaker\",\n",
        "                            \"summary\": cleaned_summary,\n",
        "                            \"gold\": gold_summary\n",
        "                        })\n",
        "\n",
        "            # ✅ Extract GLIMPSE-Unique summaries (Top 3 based on consensuality score)\n",
        "            initial_consensuality_scores = result.get(\"initial_consensuality_scores\", None)\n",
        "            if isinstance(initial_consensuality_scores, pd.Series) and not initial_consensuality_scores.empty:\n",
        "                sorted_summaries = initial_consensuality_scores.sort_values(ascending=False).index[:3]\n",
        "                for summary in sorted_summaries:\n",
        "                    cleaned_summary = clean_text(summary)\n",
        "                    if len(cleaned_summary.split()) > 5:  # ✅ Remove very short summaries\n",
        "                        rows_unique.append({\n",
        "                            \"id\": id_,\n",
        "                            \"Method\": \"GLIMPSE-Unique\",\n",
        "                            \"summary\": cleaned_summary,\n",
        "                            \"gold\": gold_summary\n",
        "                        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing {pk_file}: {e}\")\n",
        "\n",
        "# Convert lists to DataFrames\n",
        "df_speaker = pd.DataFrame(rows_speaker)\n",
        "df_unique = pd.DataFrame(rows_unique)\n",
        "\n",
        "# Save extracted and cleaned data to CSV files\n",
        "df_speaker.to_csv(output_speaker, index=False)\n",
        "df_unique.to_csv(output_unique, index=False)\n",
        "\n",
        "print(\"✅ GLIMPSE-Speaker summaries saved to:\", output_speaker)\n",
        "print(\"✅ GLIMPSE-Unique summaries saved to:\", output_unique)\n"
      ],
      "metadata": {
        "id": "kDfTTNS2oBHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/rough_pegasus_abstractive_speaker.csv\n",
        "!python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/evaluate_common_metrics_samples.py --summaries /content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/rough_pegasus_abstractive_unique.csv"
      ],
      "metadata": {
        "id": "owuRD64uoBEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the updated CSVs after running ROUGE evaluation\n",
        "output_speaker = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/rough_pegasus_abstractive_speaker.csv\"\n",
        "output_unique = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/rough_pegasus_abstractive_unique.csv\"\n",
        "\n",
        "# Load CSV files\n",
        "df_speaker = pd.read_csv(output_speaker)\n",
        "df_unique = pd.read_csv(output_unique)\n",
        "\n",
        "# Check if ROUGE scores exist\n",
        "print(\"Columns in GLIMPSE-Speaker CSV:\", df_speaker.columns)\n",
        "print(\"Columns in GLIMPSE-Unique CSV:\", df_unique.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTBOQlsCo92h",
        "outputId": "8d6cdec2-ec43-43c3-e65c-7167b375ce47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in GLIMPSE-Speaker CSV: Index(['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2',\n",
            "       'common/rougeL', 'common/rougeLsum'],\n",
            "      dtype='object')\n",
            "Columns in GLIMPSE-Unique CSV: Index(['id', 'Method', 'summary', 'gold', 'common/rouge1', 'common/rouge2',\n",
            "       'common/rougeL', 'common/rougeLsum'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only ROUGE score columns\n",
        "rouge_columns = [\"common/rouge1\", \"common/rouge2\", \"common/rougeL\", \"common/rougeLsum\"]\n",
        "\n",
        "# Compute mean ROUGE scores\n",
        "mean_speaker = df_speaker[rouge_columns].mean()\n",
        "mean_unique = df_unique[rouge_columns].mean()\n",
        "\n",
        "# Print results\n",
        "print(\"\\n📌 Mean ROUGE Scores for GLIMPSE-Speaker:\")\n",
        "print(mean_speaker)\n",
        "\n",
        "print(\"\\n📌 Mean ROUGE Scores for GLIMPSE-Unique:\")\n",
        "print(mean_unique)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z837ucCGo9zQ",
        "outputId": "5ea4c968-55fd-459d-ba4e-64fa461cb273"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📌 Mean ROUGE Scores for GLIMPSE-Speaker:\n",
            "common/rouge1       0.209226\n",
            "common/rouge2       0.027800\n",
            "common/rougeL       0.124781\n",
            "common/rougeLsum    0.124781\n",
            "dtype: float64\n",
            "\n",
            "📌 Mean ROUGE Scores for GLIMPSE-Unique:\n",
            "common/rouge1       0.208563\n",
            "common/rouge2       0.027755\n",
            "common/rougeL       0.124317\n",
            "common/rougeLsum    0.124317\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only ROUGE score columns\n",
        "rouge_columns = [\"common/rouge1\", \"common/rouge2\", \"common/rougeL\", \"common/rougeLsum\"]\n",
        "\n",
        "# Compute mean and std ROUGE scores (as decimals, not percentages)\n",
        "mean_speaker = df_speaker[rouge_columns].mean()  # Removed * 100\n",
        "std_speaker = df_speaker[rouge_columns].std()    # Removed * 100\n",
        "mean_unique = df_unique[rouge_columns].mean()    # Removed * 100\n",
        "std_unique = df_unique[rouge_columns].std()      # Removed * 100\n",
        "\n",
        "# Create a formatted table\n",
        "print(\"\\nROUGE Scores:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<12} {'GLIMPSE-Speaker':>22} {'GLIMPSE-Unique':>22}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "metrics_display = {\n",
        "    \"common/rouge1\": \"ROUGE-1\",\n",
        "    \"common/rouge2\": \"ROUGE-2\",\n",
        "    \"common/rougeL\": \"ROUGE-L\",\n",
        "    \"common/rougeLsum\": \"ROUGE-Lsum\"\n",
        "}\n",
        "\n",
        "for col in rouge_columns:\n",
        "    metric_name = metrics_display[col]\n",
        "    # Format scores to match paper style (0.XX ±0.XX)\n",
        "    speaker_score = f\"{mean_speaker[col]:.2f} ±{std_speaker[col]:.2f}\"\n",
        "    unique_score = f\"{mean_unique[col]:.2f} ±{std_unique[col]:.2f}\"\n",
        "    print(f\"{metric_name:<12} {speaker_score:>22} {unique_score:>22}\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKQ1rOyPo9wV",
        "outputId": "2d964f44-5102-48fc-febf-cdfaf2807bfb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores:\n",
            "============================================================\n",
            "Metric              GLIMPSE-Speaker         GLIMPSE-Unique\n",
            "------------------------------------------------------------\n",
            "ROUGE-1                  0.21 ±0.08             0.21 ±0.08\n",
            "ROUGE-2                  0.03 ±0.03             0.03 ±0.03\n",
            "ROUGE-L                  0.12 ±0.04             0.12 ±0.04\n",
            "ROUGE-Lsum               0.12 ±0.04             0.12 ±0.04\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing df s to for seahorse calculation\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load CSV\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/seahorse_pegasus_abstractive_speaker.csv\"\n",
        "file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/seahorse_pegasus_abstractive_unique.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename 'gold' → 'text'\n",
        "df.rename(columns={\"gold\": \"text\"}, inplace=True)\n",
        "\n",
        "# Save the updated file\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "print(\"✅ CSV column renamed successfully!\")"
      ],
      "metadata": {
        "id": "3PGV2WXOo9tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seahorse metrics calculation\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the CSV file\n",
        "#file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/seahorse_pegasus_abstractive_speaker.csv\"\n",
        "file_path = \"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/seahorse_pegasus_abstractive_unique.csv\"\n",
        "\n",
        "for question in range(1, 7):\n",
        "    print(f\"⚡ Running Seahorse for Question {question}...\")\n",
        "\n",
        "    # Reload dataset before each run to avoid missing columns\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        print(\"❌ ERROR: 'text' and 'summary' columns missing before running Seahorse. Stopping execution.\")\n",
        "        break  # Stop if important columns are missing\n",
        "\n",
        "    # Run the Seahorse evaluation script\n",
        "    !python /content/drive/MyDrive/glimpse/glimpse-mds/glimpse/evaluate/Copyof_evaluate_seahorse_metrics_samples.py \\\n",
        "        --summaries {file_path} \\\n",
        "        --question {question} \\\n",
        "        --batch_size 8 \\\n",
        "        --device cuda\n",
        "\n",
        "    # ✅ Verify columns after running Seahorse\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"✅ After Question {question}, Columns:\", df.columns.tolist())\n",
        "\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        print(\"❌ ERROR: 'text' and 'summary' columns missing after running Seahorse. Stopping execution.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "TKpsNL2Ho9qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the speaker summaries file\n",
        "#df_seahorse = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/seahorse_pegasus_abstractive_speaker.csv\")\n",
        "df_seahorse = pd.read_csv(\"/content/drive/MyDrive/glimpse/glimpse-mds/data/extension2/rsa_scores/pegasus/seahorse_pegasus_abstractive_unique.csv\")\n",
        "\n",
        "# Get SEAHORSE metrics (only the proba_1 columns)\n",
        "seahorse_cols = [col for col in df_seahorse.columns if col.startswith(\"SHMetric\") and col.endswith(\"proba_1\")]\n",
        "\n",
        "print(\"\\nSEAHORSE Metrics for GLIMPSE:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if seahorse_cols:\n",
        "    for col in seahorse_cols:\n",
        "        metric_name = col.split(\"/\")[1]\n",
        "        mean_score = df_seahorse[col].mean()\n",
        "        std_score = df_seahorse[col].std()\n",
        "        print(f\"{metric_name}: {mean_score:.2f} ± {std_score:.2f}\")\n",
        "else:\n",
        "    print(\"No SEAHORSE metrics found in the file.\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Print some basic statistics\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(f\"Number of summaries: {len(df_seahorse)}\")\n",
        "print(f\"Average summary length: {df_seahorse['summary'].str.len().mean():.2f} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmhi1bEro9nq",
        "outputId": "5af9b894-91d2-4d2a-8bac-8a1eb8bce4c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SEAHORSE Metrics for GLIMPSE:\n",
            "--------------------------------------------------\n",
            "Comprehensible: 0.64 ± 0.22\n",
            "Repetition: 0.90 ± 0.12\n",
            "Grammar: 0.45 ± 0.24\n",
            "Attribution: 0.35 ± 0.02\n",
            "Main ideas: 0.06 ± 0.06\n",
            "Conciseness: 0.13 ± 0.03\n",
            "--------------------------------------------------\n",
            "\n",
            "Basic Statistics:\n",
            "Number of summaries: 1455\n",
            "Average summary length: 431.85 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V8-dPA1io9k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7c4TTtYqo9iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsyuQ3Alo9fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSaxZh_Zo9b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWl0QUd8o9Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5Pt5MS4o9WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tOaC8pOJo9S1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}